What is Big O notation?  Big-O notation (also called "asymptotic growth" notation) is a relative representation of the complexity of an algorithm. It shows how an algorithm scales based on input size. We use it to talk about how thing scale.


What is worst case? Big-O is often used to make statements about functions that measure the worst case behavior of an algorithm. Worst case analysis gives the maximum number of basic operations that have to be performed during execution of the algorithm. It assumes that the input is in the worst possible state and maximum work has to be done to put things right.


What the heck does it mean if an operation is O(log n)? O(log n) means for every element, you're doing something that only needs to look at log N of the elements. This is usually because you know something about the elements that let you make an efficient choice (for example to reduce a search space). The most common attributes of logarithmic running-time function are that: the choice of the next element on which to perform some action is one of several possibilities, and only one will need to be chosen or the elements on which the action is performed are digits of n. Most efficient sorts are an example of this, such as merge sort. ​It is O(log n) when we do divide and conquer type of algorithms e.g binary search. Another example is quick sort where each time we divide the array into two parts and each time it takes O(N) time to find a pivot element. Hence it N O(log N)


Why do we use Big O notation to compare algorithms?  The fact is it's difficult to determine the exact runtime of an algorithm. It depends on the speed of the computer processor. So instead of talking about the runtime directly, we use Big O Notation to talk about how quickly the runtime grows depending on input size. With Big O Notation, we use the size of the input, which we call n. So we can say things like the runtime grows “on the order of the size of the input” (O(n)) or “on the order of the square of the size of the input” (O(n^2)). Our algorithm may have steps that seem expensive when n is small but are eclipsed eventually by other steps as n gets larger. For Big O Notation analysis, we care more about the stuff that grows fastest as the input grows, because everything else is quickly eclipsed as n gets very large.


What exactly would an O(n^2) operation do? O(n^2) means for every element, you're doing something with every other element, such as comparing them. Bubble sort is an example of this.


Explain the difference between O(1) vs O(n) space complexities. Let's consider a traversal algorithm for traversing a list. O(1) denotes constant space use: the algorithm allocates the same number of pointers irrespective to the list size. That will happen if we move (reuse) our pointer along the list. In contrast, O(n) denotes linear space use: the algorithm space use grows together with respect to the input size n. That will happen if let's say for some reason the algorithm needs to allocate 'N' pointers (or other variables) when traversing a list.


Provide an example of O(1) algorithm in code. Say we have an array of n elements:
int array[n];
If we wanted to access the first (or any) element of the array this would be O(1) since it doesn't matter how big the array is, it always takes the same constant time to get the first item:
x = array[0];


Explain what is an algorithm in computing? An algorithm is a well-defined computational procedure that take some value as input and generate some value as output. In simple words, it’s a sequence of computational steps that converts input into the output.


Explain what is Quick Sort algorithm? Quick Sort algorithm has the ability to sort list or queries quickly. It is based on the principle of partition exchange sort or Divide and conquer. This type of algorithm occupies less space, and it segregates the list into three main parts.
-Elements less than the Pivot element
-Pivot element
-Elements greater than the Pivot element


Explain what is time complexity of Algorithms? Time complexity of an algorithm indicates the total time needed by the program to run to completion. It is usually expressed by using the big O notation.


Mention what are the types of Notation used for Time Complexity? The types of Notations used for Time Complexity includes
Big O: It indicates “fewer than or the same as” <expression>iterations
Big Omega: It indicates “more than or same as” <expression>iterations
Big Theta: It indicates “the same as”<expression>iterations
Little Oh: It indicates “fewer than” <expression>iterations
Little Omega: It indicates “more than” <expression>iterations


Explain how binary search works? In binary search, we compare the key with the item in the middle position of the array. If the key is less than the item searched then it must lie in the lower half of the array, if the key is greater than the item searched than it should be in upper half of the array.


Explain whether it is possible to use binary search for linked lists? Since random access is not acceptable in linked list, it is impossible to reach the middle element of O(1) time. Thus, binary search is not possible for linked list.


Explain what is heap sort? Heap-sort can be defined as a comparison based sorting algorithm. It divides its input into the unsorted and sorted region, until it shrinks the unsorted region by eliminating the smallest element and moving that to the sorted region.


Explain what is Skip list? Skip list the method for data structuring, where it allows the algorithm to search, delete and insert elements in a symbol table or dictionary. In a skip list, each element is represented by a node. The search function returns the content of the value related to key. The insert operation associates a specified key with a new value, while the delete function deletes the specified key.


Explain what is Space complexity of insertion sort algorithm? Insertion sort is an in-place sorting algorithm which means that it requires no extra or little. storage. For insertion sort, it requires only single list elements to be stored out-side the initial data, making the space-complexity 0(1).


Explain what a “Hash Algorithm” is and what are they used for? “Hash Algorithm” is a hash function that takes a string of any length and decreases it to a unique fixed length string. It is used for password validity, message & data integrity and for many other cryptographic systems.


Explain how to find whether the linked list has a loop? To know whether the linked list has a loop, we will take two pointer approach. If we maintain two pointers, and we increase one pointer after processing two nodes and other after processing every node, we are likely to encounter a situation where both the pointer will be pointing to the same node. This will only occur if linked list has a loop.


Explain how encryption algorithm works? Encryption is the process of converting plaintext into a secret code format referred as “Ciphertext”. To convert the text, algorithm uses a string of bits referred as “keys” for calculations. The larger the key, the greater the number of potential patterns for creating cipher text. Most encryption algorithm use codes fixed blocks of input that have length about 64 to 128 bits, while some uses stream method.


List out some of the commonly used cryptographic algorithms? Some of the commonly used cryptographic algorithms are
3-way
Blowfish
CAST
CMEA
GOST
DES and Triple DES
IDEA
LOKI


Explain what is the difference between best case scenario and worst case scenario of an algorithm? Best case scenario: Best case scenario for an algorithm is explained as the arrangement of data for which the algorithm performs best. For example, we take a binary search, for which the best case scenario would be if the target value is at the very center of the data you are searching. The best case time complexity would be 0 (1).
Worst case scenario: It is referred for the worst set of input for a given algorithm. For example quicksort, which can perform worst if you select the largest or smallest element of a sublist for the pivot value. It will cause quicksort to degenerate to O (n2).


Explain what is Radix Sort algorithm? Radix sort puts the element in order by comparing the digits of the numbers. It is one of the linear sorting algorithms for integers.


Explain what is a recursive algorithm? Recursive algorithm is a method of solving a complicated problem by breaking a problem down into smaller and smaller sub-problems until you get the problem small enough that it can be solved easily. Usually, it involves a function calling itself.


Mention what are the three laws of recursion algorithm? All recursive algorithm must follow three laws
It should have a base case
A recursive algorithm must call itself
A recursive algorithm must change its state and move towards the base case


 Explain what is bubble sort algorithm? Bubble sort algorithm is also referred as sinking sort. In this type of sorting, the list to be sorted out compares the pair of adjacent items. If they are organized in the wrong order, it will swap the values and arrange them in the correct order.


Write a proof by induction to show the correctness of the binary search code given below: 
// Find index of x in sorted array  A[p..r]. Return -1 if x is not in A[p..r]. 
int binarySearch ( A, p, r, x ): // Pre: A[p..r] is sorted 
if p > r then return -1 
else 
q <-- (p+r)/2 
if x < A[q] then 
return binarySearch ( A, p, q-1, x) 
else if x = A[q] then 
return q 
else // x > A[q] 
return binarySearch ( A, q+1, r, x)
Base Case: Array of size one where both p=r=1 q = 1 if x = A[1] then it will return 1 or else -1 Inductive Case: Let us assume the algorithm works for all array sizes up to k. We have to prove it works for array of size k + one. Both the sub problems should work correctly since their sizes are less then k . The center q calculation and checking works correctly, if x=A[q] returns q or returns the correct solution from the sub problems. Therefore, the algorithm is correct. Hence Proved


Consider sorting n numbers stored in array A by first finding the smallest element of A and exchanging it with the element A[1]. Then find the second smallest element of A, and exchange it with A[2]. Continue in this manner for the first n-1 elements of A. Write pseudocode for this algorithm, which is known as selection sort. What loop invariant does this algorithm maintain? Why does it need to run only the first n-1 elements, rather than for all n-elements? Give the bestcase and worst-case running times of selection sort in Θ-notation.
Selection-Sort(A)
n = A.length
For j = 1 to n - 1
        smallest = j
        For i = j+1 to n
                If A[i] < A[smallest]
                        smallest = i
        Exchange A[j] with A[smallest]
The algorithm maintains the loop invariant that at the start of each iteration of the outer for loop, the subarray A[1…j-1] consists of the j-1 smallest elements in the array A[1…n], and this subarray is in sorted order. After the first n-1 elements, the subarray A[1..n-1] contains the smallest n-1 elements, sorted, and therefor element A[n] must be the largest element. The running time of the algorithm is O(n^2) for all cases.


Describe a O(n lg n) –time algorithm that, given a set S of n integers and another integer x, determines whether or not there exist two elements in S whose sum is exactly x
The following algorithm solves the problem:
1. Sort the elements in S
2. Form the set S’ = {z : z = x - y for some y subset of S}
3. Sort the elements in S’
4. Merge the sorted sets S and S’
5. There exists two elements in S whose sum is exactly x if and only if the same value appears in consecutive positions in the merged output.
To justify the claim in step 4, first observe that if any value appears twice in the merged output, it must appear in consecutive positions. Thus, we can restate the condition in step 5 as there exists two elements in S whose sum is exactly x if and only if the same value appears twice in the merged output.